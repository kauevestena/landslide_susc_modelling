# Before vs After Comparison

This document shows the expected improvements after regenerating artifacts with the fixed configuration.

---

## Dataset Configuration

| Setting | BEFORE (Current) | AFTER (Expected) | Impact |
|---------|------------------|------------------|--------|
| **Tile Size** | 512Ã—512 | 256Ã—256 | 4Ã— more tiles |
| **Tile Overlap** | 64 px | 32 px | Proportional |
| **min_valid_fraction** | 0.5-0.55 | 0.4 | More tiles pass filter |
| **positive_min_fraction** | 0.01 (1%) | 0.005 (0.5%) | More tiles accepted |
| **positive_fraction** | 0.5 (50%) | 0.3 (30%) | Better balance |

---

## Dataset Splits

### BEFORE (Current - BROKEN)
```
Training Area: 1574 Ã— 1607 pixels
Tile Size: 512 Ã— 512
Possible Positions: ~9
After Filtering: 8 tiles

Split Distribution:
â”œâ”€â”€ Train: 8 tiles (100%) âœ“
â”œâ”€â”€ Val:   0 tiles (0%)   âŒ EMPTY
â””â”€â”€ Test:  0 tiles (0%)   âŒ EMPTY

Issue: 8 Ã— 0.15 = 1.2 â†’ rounds to 0
       8 Ã— 0.20 = 1.6 â†’ rounds to 0
```

### AFTER (Expected - FIXED)
```
Training Area: 1574 Ã— 1607 pixels
Tile Size: 256 Ã— 256
Possible Positions: ~36
After Filtering: ~32 tiles (expected)

Split Distribution:
â”œâ”€â”€ Train: ~21 tiles (65%) âœ“
â”œâ”€â”€ Val:   ~5 tiles (15%)  âœ“ NOW EXISTS
â””â”€â”€ Test:  ~6 tiles (20%)  âœ“ NOW EXISTS

Fix: 32 Ã— 0.15 = 4.8 â†’ rounds to 5
     32 Ã— 0.20 = 6.4 â†’ rounds to 6
```

---

## Training Metrics

### BEFORE (Current - INVALID)
```json
{
  "train_loss": [0.523, 0.412, ...],
  "train_macro_iou": [0.45, 0.58, ...],
  "val_loss": [NaN, NaN, NaN],        âŒ No validation
  "val_macro_iou": [NaN, NaN, NaN],   âŒ No validation
  "best_epoch": 20,
  "best_val_macro_iou": NaN           âŒ Cannot select best model
}
```

**Problem:** Model cannot be properly validated or early-stopped!

### AFTER (Expected - VALID)
```json
{
  "train_loss": [0.523, 0.412, 0.350, ...],
  "train_macro_iou": [0.45, 0.58, 0.64, ...],
  "val_loss": [0.612, 0.495, 0.428, ...],      âœ“ Has values
  "val_macro_iou": [0.38, 0.51, 0.55, ...],    âœ“ Has values
  "best_epoch": 15,
  "best_val_macro_iou": 0.55                   âœ“ Proper selection
}
```

**Fixed:** Model can now be validated and best checkpoint selected!

---

## Model Predictions

### BEFORE (Current - UNREALISTIC)
```
Susceptibility Statistics:
  Mean:   0.9098  âš ï¸ WAY TOO HIGH
  Median: 0.9723  âš ï¸ WAY TOO HIGH
  Std:    0.1470
  Min:    0.0537
  Max:    1.0000

Risk Distribution:
  High Risk (>0.7):      92.38%  ðŸ”´ UNREALISTIC
  Moderate Risk (0.3-0.7): 6.71%
  Low Risk (<0.3):         0.91%

Interpretation: Model predicts almost everything as high risk!
```

### AFTER (Expected - REALISTIC)
```
Susceptibility Statistics:
  Mean:   0.35-0.45  âœ“ Reasonable
  Median: 0.30-0.50  âœ“ Reasonable
  Std:    0.20-0.30  âœ“ Better spread
  Min:    0.00-0.10
  Max:    0.90-1.00

Risk Distribution:
  High Risk (>0.7):      10-30%  âœ“ More realistic
  Moderate Risk (0.3-0.7): 30-50%
  Low Risk (<0.3):         30-50%

Interpretation: Model now produces balanced risk predictions!
```

---

## Validation Capability

### BEFORE (Current)
```
âœ… Can compute training metrics
âŒ Cannot compute validation metrics (no val split)
âŒ Cannot compute test metrics (no test split)
âŒ Cannot perform early stopping properly
âŒ Cannot select best model checkpoint
âŒ Cannot evaluate generalization
âŒ Cannot compute AUROC/AUPRC with ground truth
âŒ Model may be severely overfitting
```

### AFTER (Expected)
```
âœ… Can compute training metrics
âœ… Can compute validation metrics (has val split)
âœ… Can compute test metrics (has test split)
âœ… Can perform early stopping properly
âœ… Can select best model checkpoint
âœ… Can evaluate generalization
âœ… Can compute AUROC/AUPRC (if ground truth available)
âœ… Model overfitting is detected and prevented
```

---

## File Structure

### BEFORE (Current - Incomplete)
```
artifacts/
â”œâ”€â”€ tiles/
â”‚   â”œâ”€â”€ train/       [8 files]  âœ“
â”‚   â”œâ”€â”€ val/         [EMPTY]    âŒ
â”‚   â””â”€â”€ test/        [EMPTY]    âŒ
â”œâ”€â”€ labels/
â”‚   â”œâ”€â”€ train/       [8 files]  âœ“
â”‚   â”œâ”€â”€ val/         [EMPTY]    âŒ
â”‚   â””â”€â”€ test/        [EMPTY]    âŒ
â””â”€â”€ splits/
    â”œâ”€â”€ splits.json
    â”‚   {"train": 8, "val": 0, "test": 0}  âŒ
    â””â”€â”€ dataset_summary.json
```

### AFTER (Expected - Complete)
```
artifacts/
â”œâ”€â”€ tiles/
â”‚   â”œâ”€â”€ train/       [~21 files] âœ“
â”‚   â”œâ”€â”€ val/         [~5 files]  âœ“ NOW EXISTS
â”‚   â””â”€â”€ test/        [~6 files]  âœ“ NOW EXISTS
â”œâ”€â”€ labels/
â”‚   â”œâ”€â”€ train/       [~21 files] âœ“
â”‚   â”œâ”€â”€ val/         [~5 files]  âœ“ NOW EXISTS
â”‚   â””â”€â”€ test/        [~6 files]  âœ“ NOW EXISTS
â””â”€â”€ splits/
    â”œâ”€â”€ splits.json
    â”‚   {"train": 21, "val": 5, "test": 6}  âœ“
    â””â”€â”€ dataset_summary.json
```

---

## Evaluation Report

### BEFORE (Current)
```
ðŸ”´ Critical Issues Detected:

1. Only 8 training tiles created
   âŒ No validation split (0 tiles)
   âŒ No test split (0 tiles)
   âŒ Cannot evaluate model properly

2. Extreme susceptibility predictions
   âš ï¸  92.4% of area classified as HIGH RISK
   âš ï¸  Mean probability: 0.91
   âš ï¸  Likely indicates calibration/training issues

3. No ground truth for test area
   âŒ Cannot compute accuracy metrics
   âŒ Only spatial statistics available

Status: âš ï¸ Model requires retraining
```

### AFTER (Expected)
```
âœ… Dataset Structure Verified:

1. Proper train/val/test splits
   âœ“ Training: 21 tiles
   âœ“ Validation: 5 tiles
   âœ“ Test: 6 tiles

2. Realistic susceptibility predictions
   âœ“ 10-30% of area classified as HIGH RISK
   âœ“ Mean probability: 0.35-0.45
   âœ“ Model produces balanced predictions

3. Validation metrics available
   âœ“ Val IoU: 0.50-0.60
   âœ“ Val AUROC: 0.75-0.85 (if ground truth)
   âœ“ Early stopping effective

Status: âœ… Model properly trained and validated
```

---

## Performance Metrics

| Metric | BEFORE | AFTER (Expected) | Status |
|--------|--------|------------------|--------|
| **Train Tiles** | 8 | ~21 | âœ… Improved |
| **Val Tiles** | 0 | ~5 | âœ… Fixed |
| **Test Tiles** | 0 | ~6 | âœ… Fixed |
| **Val Loss** | NaN | 0.40-0.50 | âœ… Fixed |
| **Val IoU** | NaN | 0.50-0.60 | âœ… Fixed |
| **Mean Susceptibility** | 0.91 | 0.35-0.45 | âœ… Fixed |
| **High Risk %** | 92% | 10-30% | âœ… Fixed |
| **Can Validate** | No | Yes | âœ… Fixed |
| **Can Early Stop** | No | Yes | âœ… Fixed |

---

## Root Cause Summary

### The Problem
```
Input Area: 1574Ã—1607 = 2.5M pixels
Tile Size: 512Ã—512 = 262K pixels per tile
Possible Tiles: ~9 positions
After Filtering: 8 tiles

8 tiles Ã— 0.15 = 1.2 â†’ rounds to 0 val tiles âŒ
8 tiles Ã— 0.20 = 1.6 â†’ rounds to 0 test tiles âŒ
```

### The Solution
```
Input Area: 1574Ã—1607 = 2.5M pixels  (unchanged)
Tile Size: 256Ã—256 = 65K pixels per tile  (4Ã— smaller)
Possible Tiles: ~36 positions
After Filtering: ~32 tiles (with relaxed thresholds)

32 tiles Ã— 0.15 = 4.8 â†’ rounds to 5 val tiles âœ“
32 tiles Ã— 0.20 = 6.4 â†’ rounds to 6 test tiles âœ“
```

---

## How to Regenerate

Run the automated script:
```bash
./regenerate_artifacts.sh
```

Or manually:
```bash
.venv/bin/python -m src.main_pipeline --force_recreate
```

**Duration:** ~30-55 minutes  
**Result:** All issues should be resolved!

---

## Verification Commands

After regeneration, verify the fixes:

```bash
# Check tile counts
cat artifacts/splits/dataset_summary.json | jq '.tile_counts'

# Count files
ls artifacts/tiles/train/*.npy | wc -l
ls artifacts/tiles/val/*.npy | wc -l
ls artifacts/tiles/test/*.npy | wc -l

# Check validation metrics
cat artifacts/experiments/training_metrics.json | jq '.val_macro_iou[-1]'

# Check susceptibility statistics
cat outputs/evaluation/output_statistics.json | jq '.susceptibility_stats.mean'

# Check risk distribution
cat outputs/evaluation/output_statistics.json | jq '.risk_distribution.high_risk'
```

---

**Summary:** Configuration is fixed âœ…, now awaiting artifact regeneration with `--force_recreate`
